{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5e7a07-109a-4ec6-b42a-3d012aff414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab19dde-f063-4f8e-95c9-fa6d306d1b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "class split information: (tensor([6, 1]), tensor([5, 3]), tensor([0, 4]))\n"
     ]
    }
   ],
   "source": [
    "from begin.algorithms.bare.nc_class import NCClassILBareTrainer\n",
    "from begin.scenarios.nodes import NCScenarioLoader\n",
    "\n",
    "scenario = NCScenarioLoader(dataset_name='cora', num_tasks=3, metric='accuracy', save_path='data', incr_type='class', task_shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb98f886-b26d-4961-9b55-e670cdcf59bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "begin.algorithms.bare.nc_class.NCClassILBareTrainer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCClassILBareTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b929ce6-d489-478f-a794-80953c96b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dgl.nn import GraphConv, SumPooling, AvgPooling, MaxPooling\n",
    "import torch.nn.functional as F\n",
    "from dgl.base import DGLError\n",
    "from dgl.utils import expand_as_pair\n",
    "import dgl.function as fn\n",
    "from torch_scatter import segment_csr\n",
    "\n",
    "class AdaptiveLinear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, accum=True):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_channels, out_channels, bias)\n",
    "        self.bias = bias\n",
    "        self.accum = accum\n",
    "        self.num_outputs = out_channels\n",
    "        self.output_masks = None\n",
    "        self.observed = torch.zeros(out_channels, dtype=torch.bool)\n",
    "        \n",
    "    def observe_outputs(self, new_outputs, verbose=True):\n",
    "        device = self.lin.weight.data.device\n",
    "        new_outputs = torch.unique(new_outputs)\n",
    "        new_num_outputs = max(self.num_outputs, new_outputs.max() + 1)\n",
    "        new_output_mask = torch.zeros(new_num_outputs, dtype=torch.bool).to(device)\n",
    "        new_output_mask[new_outputs] = True\n",
    "        \n",
    "        prv_observed = self.observed\n",
    "        \n",
    "        if self.output_masks is None: self.output_masks = [new_output_mask]\n",
    "        else:\n",
    "            if new_num_outputs > self.num_outputs:\n",
    "                self.output_masks = [torch.cat((output_mask, torch.zeros(new_num_outputs - self.num_outputs, dtype=torch.bool).to(device)), dim=-1) for output_mask in self.output_masks]\n",
    "            if self.accum: self.output_masks.append(self.output_masks[-1] | new_output_mask)\n",
    "            else: self.output_masks.append(new_output_mask)    \n",
    "        \n",
    "        if new_num_outputs > self.num_outputs:\n",
    "            prev_weight, prev_bias = self.lin.weight.data[prv_observed], (self.lin.bias.data[prv_observed] if self.bias else None)\n",
    "            self.observed = torch.cat((self.observed.to(device), torch.zeros(new_num_outputs - self.num_outputs, dtype=torch.bool).to(device)), dim=-1)\n",
    "            self.lin = nn.Linear(in_features, new_num_outputs, bias=self.bias)\n",
    "            self.lin.weight.data[self.observed] = prev_weight\n",
    "            if self.bias: self.lin.bias.data[self.observed] = prev_bias    \n",
    "            self.num_outputs = new_num_outputs\n",
    "        self.observed = self.observed.to(device) | new_output_mask    \n",
    "        \n",
    "        # print(self.accum)\n",
    "        # print(self.output_masks)\n",
    "    def get_output_mask(self, task_ids=None):\n",
    "        if task_ids is None: return self.output_masks[-1]\n",
    "        else:\n",
    "            mask = torch.zeros(task_ids.shape[0], self.num_outputs).bool().to(task_ids.device)\n",
    "            observed_mask = task_ids < len(self.output_masks)\n",
    "            \n",
    "            # print(mask.device, observed_mask.device, task_ids.device)\n",
    "            mask[observed_mask] = torch.stack(self.output_masks, dim=0)[task_ids[observed_mask]]\n",
    "            return mask\n",
    "    \n",
    "    def forward(self, x, task_masks=None):\n",
    "        out = self.lin(x)\n",
    "        if task_masks is None:\n",
    "            out[..., ~self.observed] = -1e12\n",
    "        else:\n",
    "            out[~task_masks] = -1e12\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_classes, n_hidden, activation = F.relu, dropout=0.0, n_layers=3, incr_type='class', use_classifier=True):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            in_hidden = n_hidden if i > 0 else in_feats\n",
    "            out_hidden = n_hidden\n",
    "            self.convs.append(GraphConv(in_hidden, out_hidden, \"both\", bias=False, allow_zero_in_degree=True))\n",
    "            self.norms.append(nn.BatchNorm1d(out_hidden))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "        if use_classifier:\n",
    "            self.classifier = AdaptiveLinear(n_hidden, n_classes, bias=True, accum = False if incr_type == 'task' else True)\n",
    "        else:\n",
    "            self.classifier = None\n",
    "            \n",
    "    def forward(self, graph, feat, task_masks=None):\n",
    "        h = feat\n",
    "        h = self.dropout(h)\n",
    "        for i in range(self.n_layers):\n",
    "            conv = self.convs[i](graph, h)\n",
    "            h = conv\n",
    "            h = self.norms[i](h)\n",
    "            h = self.activation(h)\n",
    "            h = self.dropout(h)\n",
    "        if self.classifier is not None:\n",
    "            h = self.classifier(h, task_masks)\n",
    "        return h\n",
    "    \n",
    "    def observe_labels(self, new_labels, verbose=True):\n",
    "        self.classifier.observe_outputs(new_labels, verbose=verbose)\n",
    "    \n",
    "    def get_observed_labels(self, tid=None):\n",
    "        if tid is None or tid < 0:\n",
    "            return self.classifier.observed\n",
    "        else:\n",
    "            return self.classifier.output_masks[tid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af416dda-b800-4828-b446-6c857be8f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(scenario.num_feats, scenario.num_classes, 256, dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bc49a2-90cd-4d3d-ac0e-6828eb9b2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = NCClassILBareTrainer(model = model,\n",
    "                                 scenario = scenario,\n",
    "                                 optimizer_fn = lambda x: torch.optim.Adam(x, lr=1e-3, weight_decay=0),\n",
    "                                 loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1),\n",
    "                                 device = torch.device('cuda:0'),\n",
    "                                 scheduler_fn = lambda x: torch.optim.lr_scheduler.ReduceLROnPlateau(x, mode='min', patience=20, min_lr=1e-3 * 0.001 * 2., verbose=True),\n",
    "                                 benchmark = True, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cff85c-9d75-43d1-8bea-b92ae4305826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id: 0 Epoch #0: train_acc: 0.7 val_acc: 0.5538 train_loss: 0.6196 val_loss: 0.6893\n",
      "task_id: 0 Epoch #10: train_acc: 1.0 val_acc: 0.5538 train_loss: 0.0004 val_loss: 0.7089\n",
      "task_id: 0 Epoch #20: train_acc: 1.0 val_acc: 0.5538 train_loss: 0.0 val_loss: 0.866\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "task_id: 0 Epoch #30: train_acc: 1.0 val_acc: 0.5538 train_loss: 0.0 val_loss: 0.9603\n",
      "task_id: 0 Epoch #40: train_acc: 1.0 val_acc: 0.5538 train_loss: 0.0 val_loss: 0.944\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "task_id: 0 Epoch #50: train_acc: 1.0 val_acc: 0.5538 train_loss: 0.0 val_loss: 0.8493\n",
      "task_id: 0 Epoch #60: train_acc: 1.0 val_acc: 0.5692 train_loss: 0.0 val_loss: 0.6745\n",
      "task_id: 0 Epoch #70: train_acc: 1.0 val_acc: 0.7077 train_loss: 0.0 val_loss: 0.4325\n",
      "task_id: 0 Epoch #80: train_acc: 1.0 val_acc: 0.8769 train_loss: 0.0 val_loss: 0.228\n",
      "task_id: 0 Epoch #90: train_acc: 1.0 val_acc: 0.9385 train_loss: 0.0 val_loss: 0.132\n",
      "task_id: 0 Epoch #100: train_acc: 1.0 val_acc: 0.9538 train_loss: 0.0 val_loss: 0.1199\n",
      "task_id: 0 Epoch #110: train_acc: 1.0 val_acc: 0.9538 train_loss: 0.0 val_loss: 0.1314\n",
      "Epoch   120: reducing learning rate of group 0 to 2.0000e-06.\n",
      "task_id: 1 Epoch #0: train_acc: 0.3 val_acc: 0.5953 train_loss: 1.4208 val_loss: 1.1675\n",
      "task_id: 1 Epoch #10: train_acc: 1.0 val_acc: 0.8698 train_loss: 0.007 val_loss: 0.5304\n",
      "task_id: 1 Epoch #20: train_acc: 1.0 val_acc: 0.8884 train_loss: 0.0005 val_loss: 0.4456\n",
      "task_id: 1 Epoch #30: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0002 val_loss: 0.4344\n",
      "task_id: 1 Epoch #40: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4279\n",
      "task_id: 1 Epoch #50: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4237\n",
      "task_id: 1 Epoch #60: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4215\n",
      "task_id: 1 Epoch #70: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4208\n",
      "task_id: 1 Epoch #80: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4209\n",
      "task_id: 1 Epoch #90: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4215\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-04.\n",
      "task_id: 1 Epoch #100: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4215\n",
      "task_id: 1 Epoch #110: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4216\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0000e-05.\n",
      "task_id: 1 Epoch #120: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4216\n",
      "task_id: 1 Epoch #130: train_acc: 1.0 val_acc: 0.907 train_loss: 0.0001 val_loss: 0.4216\n",
      "Epoch   133: reducing learning rate of group 0 to 2.0000e-06.\n",
      "task_id: 2 Epoch #0: train_acc: 0.05 val_acc: 0.1197 train_loss: 2.6972 val_loss: 2.1457\n",
      "task_id: 2 Epoch #10: train_acc: 1.0 val_acc: 0.7394 train_loss: 0.0387 val_loss: 0.7946\n",
      "task_id: 2 Epoch #20: train_acc: 1.0 val_acc: 0.8662 train_loss: 0.0017 val_loss: 0.5172\n",
      "task_id: 2 Epoch #30: train_acc: 1.0 val_acc: 0.8732 train_loss: 0.0003 val_loss: 0.4644\n",
      "task_id: 2 Epoch #40: train_acc: 1.0 val_acc: 0.8803 train_loss: 0.0002 val_loss: 0.4495\n",
      "task_id: 2 Epoch #50: train_acc: 1.0 val_acc: 0.8803 train_loss: 0.0001 val_loss: 0.4444\n",
      "task_id: 2 Epoch #60: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4433\n",
      "task_id: 2 Epoch #70: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4441\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-04.\n",
      "task_id: 2 Epoch #80: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4455\n",
      "task_id: 2 Epoch #90: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4455\n",
      "task_id: 2 Epoch #100: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4456\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "task_id: 2 Epoch #110: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4456\n",
      "task_id: 2 Epoch #120: train_acc: 1.0 val_acc: 0.8873 train_loss: 0.0001 val_loss: 0.4456\n",
      "Epoch   122: reducing learning rate of group 0 to 2.0000e-06.\n",
      "init_acc: [0.         0.75592417 0.         0.37266356]\n",
      "algo_acc_mat: [[0.9935484  0.         0.         0.17990655]\n",
      " [0.37419355 0.90758294 0.         0.5151869 ]\n",
      " [0.         0.54028434 0.8781362  0.5525701 ]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'exp_AP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3fe36c53e985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_per_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/22-graph-continual/BeGin/begin/trainers/nodes.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epoch_per_task)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init_acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'algo_acc_mat:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo_acc_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AP:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_AP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AF:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_AF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FWT:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_FWT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'exp_AP'"
     ]
    }
   ],
   "source": [
    "results = benchmark.run(epoch_per_task = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538eeb2d-49b2-4aae-9d6e-3d1642c9d573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch1.7.1_p37)",
   "language": "python",
   "name": "conda_pytorch1.7.1_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
